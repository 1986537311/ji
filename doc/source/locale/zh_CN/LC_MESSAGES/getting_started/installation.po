# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-10-16 10:33+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/getting_started/installation.rst:5
msgid "Installation"
msgstr ""

#: ../../source/getting_started/installation.rst:6
msgid ""
"Xinference can be installed with ``pip`` on Linux, Windows, and macOS. To"
" run models using Xinference, you will need to install the backend "
"corresponding to the type of model you intend to serve."
msgstr ""

#: ../../source/getting_started/installation.rst:8
msgid ""
"If you aim to serve all supported models, you can install all the "
"necessary dependencies with a single command::"
msgstr ""

#: ../../source/getting_started/installation.rst:13
msgid ""
"If you want to serve models in GGML format, it's advised to install the "
"GGML dependencies manually based on your hardware specifications to "
"enable acceleration. For more details, see the :ref:`installation_ggml` "
"section."
msgstr ""

#: ../../source/getting_started/installation.rst:16
msgid ""
"If you want to install only the necessary backends, here's a breakdown of"
" how to do it."
msgstr ""

#: ../../source/getting_started/installation.rst:19
msgid "Transformers Backend"
msgstr ""

#: ../../source/getting_started/installation.rst:20
msgid ""
"PyTorch (transformers) supports the inference of most state-of-art "
"models. It is the default backend for models in PyTorch format::"
msgstr ""

#: ../../source/getting_started/installation.rst:26
msgid "vLLM Backend"
msgstr ""

#: ../../source/getting_started/installation.rst:27
msgid ""
"vLLM is a fast and easy-to-use library for LLM inference and serving. "
"Xinference will choose vLLM as the backend to achieve better throughput "
"when the following conditions are met:"
msgstr ""

#: ../../source/getting_started/installation.rst:29
msgid "The model format is PyTorch"
msgstr ""

#: ../../source/getting_started/installation.rst:30
msgid "The quantization method is none"
msgstr ""

#: ../../source/getting_started/installation.rst:31
msgid "The system is Linux and has at least one CUDA device"
msgstr ""

#: ../../source/getting_started/installation.rst:32
msgid "The model is within the list of models supported by vLLM."
msgstr ""

#: ../../source/getting_started/installation.rst:34
msgid "Currently, supported models include:"
msgstr ""

#: ../../source/getting_started/installation.rst:36
msgid "``llama-2``, ``llama-2-chat``"
msgstr ""

#: ../../source/getting_started/installation.rst:37
msgid "``baichuan``, ``baichuan-chat``"
msgstr ""

#: ../../source/getting_started/installation.rst:38
msgid "``internlm``, ``internlm-20b``, ``internlm-chat``, ``internlm-chat-20b``"
msgstr ""

#: ../../source/getting_started/installation.rst:39
msgid "``vicuna-v1.3``, ``vicuna-v1.5``"
msgstr ""

#: ../../source/getting_started/installation.rst:41
msgid "To install Xinference and vLLM::"
msgstr ""

#: ../../source/getting_started/installation.rst:48
msgid "GGML Backend"
msgstr ""

#: ../../source/getting_started/installation.rst:49
msgid ""
"It's advised to install the GGML dependencies manually based on your "
"hardware specifications to enable acceleration."
msgstr ""

#: ../../source/getting_started/installation.rst:51
msgid "Initial setup::"
msgstr ""

#: ../../source/getting_started/installation.rst:56
msgid "Hardware-Specific installations:"
msgstr ""

#: ../../source/getting_started/installation.rst:58
msgid "Apple Silicon::"
msgstr ""

#: ../../source/getting_started/installation.rst:62
msgid "Nvidia cards::"
msgstr ""

#: ../../source/getting_started/installation.rst:66
msgid "AMD cards::"
msgstr ""

