# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-04-02 15:27+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.14.0\n"

#: ../../source/getting_started/installation.rst:5
msgid "Installation"
msgstr "安装"

#: ../../source/getting_started/installation.rst:6
msgid ""
"Xinference can be installed with ``pip`` on Linux, Windows, and macOS. To"
" run models using Xinference, you will need to install the backend "
"corresponding to the type of model you intend to serve."
msgstr ""
"Xinference 在 Linux, Windows, MacOS 上都可以通过 ``pip`` 来安装。如果需要"
"使用 Xinference 进行模型推理，可以根据不同的模型指定不同的引擎。"

#: ../../source/getting_started/installation.rst:8
msgid ""
"If you aim to serve all supported models, you can install all the "
"necessary dependencies with a single command::"
msgstr "如果你希望能够推理所有支持的模型，可以用以下命令安装所有需要的依赖："

#: ../../source/getting_started/installation.rst:13
msgid ""
"If you want to serve models in GGML format, it's advised to install the "
"GGML dependencies manually based on your hardware specifications to "
"enable acceleration. For more details, see the :ref:`installation_ggml` "
"section."
msgstr ""
"如果你想使用 GGML 格式的模型，建议根据当前使用的硬件手动安装所需要的依赖"
"，以充分利用硬件的加速能力。更多细节可以参考 :ref:`installation_ggml` "
"这一章节。"

#: ../../source/getting_started/installation.rst:16
msgid ""
"If you want to install only the necessary backends, here's a breakdown of"
" how to do it."
msgstr "如果你只想安装必要的依赖，接下来是如何操作的详细步骤。"

#: ../../source/getting_started/installation.rst:19
msgid "Transformers Backend"
msgstr "Transformers 引擎"

#: ../../source/getting_started/installation.rst:20
msgid ""
"PyTorch (transformers) supports the inference of most state-of-art "
"models. It is the default backend for models in PyTorch format::"
msgstr ""
"PyTorch(transformers) 引擎支持几乎有所的最新模型，这是 Pytorch 模型默认"
"使用的引擎："

#: ../../source/getting_started/installation.rst:26
msgid "vLLM Backend"
msgstr "vLLM 引擎"

#: ../../source/getting_started/installation.rst:27
msgid ""
"vLLM is a fast and easy-to-use library for LLM inference and serving. "
"Xinference will choose vLLM as the backend to achieve better throughput "
"when the following conditions are met:"
msgstr ""
"vLLM 是一个支持高并发的高性能大模型推理引擎。当满足以下条件时，Xinference"
" 会自动选择 vllm 作为引擎来达到更高的吞吐量："

#: ../../source/getting_started/installation.rst:29
msgid "The model format is ``pytorch``, ``gptq`` or ``awq``."
msgstr "模型格式为 ``pytorch`` ， ``gptq`` 或者 ``awq`` 。"

#: ../../source/getting_started/installation.rst:30
msgid "When the model format is ``pytorch``, the quantization is ``none``."
msgstr "当模型格式为 ``pytorch`` 时，量化选项需为 ``none`` 。"

#: ../../source/getting_started/installation.rst:31
msgid ""
"When the model format is ``gptq`` or ``awq``, the quantization is "
"``Int4``."
msgstr "当模型格式为 ``gptq`` 或 ``awq`` 时，量化选项需为 ``Int4`` 。"

#: ../../source/getting_started/installation.rst:32
msgid "The system is Linux and has at least one CUDA device"
msgstr "操作系统为 Linux 并且至少有一个支持 CUDA 的设备"

#: ../../source/getting_started/installation.rst:33
msgid ""
"The model family (for custom models) / model name (for builtin models) is"
" within the list of models supported by vLLM"
msgstr "自定义模型的 ``model_family`` 字段和内置模型的 ``model_name`` 字段在 vLLM"
" 的支持列表中。"

#: ../../source/getting_started/installation.rst:35
msgid "Currently, supported models include:"
msgstr "目前，支持的模型包括："

#: ../../source/getting_started/installation.rst:39
msgid "``llama-2``, ``llama-2-chat``"
msgstr "``llama-2``, ``llama-2-chat``"

#: ../../source/getting_started/installation.rst:40
msgid "``baichuan``, ``baichuan-chat``, ``baichuan-2-chat``"
msgstr "``baichuan``, ``baichuan-chat``, ``baichuan-2-chat``"

#: ../../source/getting_started/installation.rst:41
msgid ""
"``internlm-16k``, ``internlm-chat-7b``, ``internlm-chat-8k``, ``internlm-"
"chat-20b``"
msgstr "``internlm-16k``, ``internlm-chat-7b``, ``internlm-chat-8k``, ``internlm-"
"chat-20b``"

#: ../../source/getting_started/installation.rst:42
msgid "``mistral-v0.1``, ``mistral-instruct-v0.1``, ``mistral-instruct-v0.2``"
msgstr ""

#: ../../source/getting_started/installation.rst:43
msgid "``Yi``, ``Yi-chat``"
msgstr "``Yi``, ``Yi-chat``"

#: ../../source/getting_started/installation.rst:44
msgid "``code-llama``, ``code-llama-python``, ``code-llama-instruct``"
msgstr "``code-llama``, ``code-llama-python``, ``code-llama-instruct``"

#: ../../source/getting_started/installation.rst:45
msgid "``vicuna-v1.3``, ``vicuna-v1.5``"
msgstr "``vicuna-v1.3``, ``vicuna-v1.5``"

#: ../../source/getting_started/installation.rst:46
msgid "``qwen-chat``"
msgstr "``qwen-chat``"

#: ../../source/getting_started/installation.rst:47
msgid "``mixtral-instruct-v0.1``"
msgstr "``mistral-instruct-v0.1``"

#: ../../source/getting_started/installation.rst:48
msgid "``chatglm3``, ``chatglm3-32k``, ``chatglm3-128k``"
msgstr ""

#: ../../source/getting_started/installation.rst:49
msgid "``deepseek-chat``, ``deepseek-coder-instruct``"
msgstr ""

#: ../../source/getting_started/installation.rst:50
msgid "``qwen1.5-chat``"
msgstr "``qwen1.5-chat``"

#: ../../source/getting_started/installation.rst:51
msgid "``gemma-it``"
msgstr ""

#: ../../source/getting_started/installation.rst:52
msgid "``orion-chat``, ``orion-chat-rag``"
msgstr ""

#: ../../source/getting_started/installation.rst:55
msgid "To install Xinference and vLLM::"
msgstr "安装 xinference 和 vLLM："

#: ../../source/getting_started/installation.rst:62
msgid "GGML Backend"
msgstr "GGML 引擎"

#: ../../source/getting_started/installation.rst:63
msgid ""
"It's advised to install the GGML dependencies manually based on your "
"hardware specifications to enable acceleration."
msgstr ""
"当使用 GGML 引擎时，建议根据当前使用的硬件手动安装依赖，从而获得最佳的"
"加速效果。"

#: ../../source/getting_started/installation.rst:65
msgid "Initial setup::"
msgstr "初始步骤："

#: ../../source/getting_started/installation.rst:70
msgid "Hardware-Specific installations:"
msgstr "不同硬件的安装方式："

#: ../../source/getting_started/installation.rst:72
msgid "Apple Silicon::"
msgstr "Apple M系列"

#: ../../source/getting_started/installation.rst:76
msgid "Nvidia cards::"
msgstr "英伟达显卡："

#: ../../source/getting_started/installation.rst:80
msgid "AMD cards::"
msgstr "AMD 显卡："

#~ msgid "The quantization method is GPTQ 4 bit or none"
#~ msgstr "量化方式必须是 GPTQ 4 bit 或者 none"

#~ msgid "``chatglm3``"
#~ msgstr "``chatglm3``"

