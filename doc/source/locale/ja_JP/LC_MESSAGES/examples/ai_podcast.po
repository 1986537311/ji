# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-11-01 10:48+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: ja_JP\n"
"Language-Team: ja_JP <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/examples/ai_podcast.rst:5
msgid "Example: AI Podcast üéô"
msgstr ""

#: ../../source/examples/ai_podcast.rst:7
msgid "**Description**:"
msgstr ""

#: ../../source/examples/ai_podcast.rst:9
msgid "üéôÔ∏èAI Podcast - Voice Conversations with Multiple Agents on M2 Max üíª"
msgstr ""

#: ../../source/examples/ai_podcast.rst:11
msgid "**Support Language** :"
msgstr ""

#: ../../source/examples/ai_podcast.rst:13
msgid "English (AI_Podcast.py)"
msgstr ""

#: ../../source/examples/ai_podcast.rst:15
msgid "Chinese (AI_Podcast_ZH.py)"
msgstr ""

#: ../../source/examples/ai_podcast.rst:17
msgid "**Used Technology (EN version)** :"
msgstr ""

#: ../../source/examples/ai_podcast.rst:19
msgid ""
"@ `OpenAI <https://twitter.com/OpenAI>`_ 's `whisper "
"<https://pypi.org/project/openai-whisper/>`_"
msgstr ""

#: ../../source/examples/ai_podcast.rst:21
msgid ""
"@ `ggerganov <https://twitter.com/ggerganov>`_ 's `ggml "
"<https://github.com/ggerganov/ggml>`_"
msgstr ""

#: ../../source/examples/ai_podcast.rst:23
msgid ""
"@ `WizardLM_AI <https://twitter.com/WizardLM_AI>`_ 's `wizardlm v1.0 "
"<https://huggingface.co/WizardLM>`_"
msgstr ""

#: ../../source/examples/ai_podcast.rst:25
msgid ""
"@ `lmsysorg <https://twitter.com/lmsysorg>`_ 's `vicuna v1.3 "
"<https://huggingface.co/lmsys/vicuna-7b-v1.3>`_"
msgstr ""

#: ../../source/examples/ai_podcast.rst:27
msgid "@ `Xinference <https://github.com/xorbitsai/inference>`_ as a launcher"
msgstr ""

#: ../../source/examples/ai_podcast.rst:29
msgid "**Detailed Explanation on the Demo Functionality** :"
msgstr ""

#: ../../source/examples/ai_podcast.rst:31
msgid ""
"Generate the Wizardlm Model and Vicuna Model when the program is "
"launching with Xorbits Inference. Initiate the Chatroom by giving the two"
" chatbot their names and telling them that there is a human user called "
"\"username\", where \"username\" is given by user's input. Initialize a "
"empty chat history for the chatroom."
msgstr ""

#: ../../source/examples/ai_podcast.rst:35
msgid ""
"Use Audio device to store recording into file, and transcribe the file "
"using OpenAI's Whisper to receive a human readable text as string."
msgstr ""

#: ../../source/examples/ai_podcast.rst:37
msgid ""
"Based on the input message string, determine which agents the user want "
"to talk to. Call the target agents and parse in the input string and chat"
" history for the model to generate."
msgstr ""

#: ../../source/examples/ai_podcast.rst:40
msgid ""
"When the responses are ready, use Macos's \"Say\" Command to produce "
"audio through speaker. Each agents have their own voice while speaking."
msgstr ""

#: ../../source/examples/ai_podcast.rst:43
msgid ""
"Store the user input and the agent response into chat history, and "
"recursively looping the program until user explicitly says words like "
"\"see you\" in their responses."
msgstr ""

#: ../../source/examples/ai_podcast.rst:46
msgid "**Highlight Features with Xinference** :"
msgstr ""

#: ../../source/examples/ai_podcast.rst:48
msgid ""
"With Xinference's distributed system, we can easily deploy two different "
"models in the same session and in the same \"chatroom\". With enough "
"resources, the framework can deploy any amount of models you like at the "
"same time."
msgstr ""

#: ../../source/examples/ai_podcast.rst:51
msgid ""
"With Xinference, you can deploy the model easily by just adding a few "
"lines of code. For examples, for launching the vicuna model in the demo, "
"just by::"
msgstr ""

#: ../../source/examples/ai_podcast.rst:68
msgid ""
"Then, the Xinference client will handle \"target model downloading and "
"caching\", \"set up environment and process for the model\", and \"run "
"the service at selected endpoint. \" You are now ready to play with your "
"llm model."
msgstr ""

#: ../../source/examples/ai_podcast.rst:71
msgid "**Original Demo Video** :"
msgstr ""

#: ../../source/examples/ai_podcast.rst:73
msgid ""
"`üéôÔ∏èAI Podcast - Voice Conversations with Multiple Agents on M2 Maxüíªüî•ü§ñ "
"<https://twitter.com/yichaocheng/status/1679129417778442240>`_"
msgstr ""

#: ../../source/examples/ai_podcast.rst:75
msgid "**Source Code** :"
msgstr ""

#: ../../source/examples/ai_podcast.rst:77
msgid ""
"`AI_Podcast "
"<https://github.com/xorbitsai/inference/blob/main/examples/AI_podcast.py>`_"
" (English Version)"
msgstr ""

#: ../../source/examples/ai_podcast.rst:79
msgid ""
"`AI_Podcast_ZH "
"<https://github.com/xorbitsai/inference/blob/main/examples/AI_podcast_ZH.py>`"
" (Chinese Version)"
msgstr ""

#~ msgid "AI_Podcast_ZH (Chinese Version)"
#~ msgstr ""

