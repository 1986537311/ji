# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-01-02 17:07+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: ja_JP\n"
"Language-Team: ja_JP <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/getting_started/troubleshooting.rst:5
msgid "Troubleshooting"
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:9
msgid "No huggingface repo access"
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:11
msgid ""
"Sometimes, you may face errors accessing huggingface models, such as the "
"following message when accessing `llama2`:"
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:18
msgid ""
"This typically indicates either a lack of access rights to the repository"
" or missing huggingface access tokens. The following sections provide "
"guidance on addressing these issues."
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:22
msgid "Get access to the huggingface repo"
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:24
msgid ""
"To obtain access, navigate to the desired huggingface repository and "
"agree to its terms and conditions. As an illustration, for the `llama2` "
"model, you can use this link: `https://huggingface.co/meta-llama/Llama-2"
"-7b-hf <https://huggingface.co/meta-llama/Llama-2-7b-hf>`_."
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:29
msgid "Set up credentials to access huggingface"
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:31
msgid ""
"Your credential to access huggingface can be found online at "
"`https://huggingface.co/settings/tokens "
"<https://huggingface.co/settings/tokens>`_."
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:33
msgid ""
"You can set the token as an environmental variable, with ``export "
"HUGGING_FACE_HUB_TOKEN=your_token_here``."
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:36
msgid "Download models from ModelScope"
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:38
msgid ""
"When the network connection to HuggingFace is blocked, you can also "
"choose to download models from ModelScope, especially for Chinese users. "
"For a detailed list of supported models and settings, please refer to "
":ref:`models_download`."
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:43
msgid "Incompatibility Between NVIDIA Driver and PyTorch Version"
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:45
msgid "If you are using a NVIDIA GPU, you may face the following error:"
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:56
msgid ""
"This typically indicates that your CUDA driver version is not compatible "
"with the PyTorch version you are using."
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:58
msgid ""
"Go to `https://pytorch.org <https://pytorch.org>`_ to install a PyTorch "
"version that has been compiled with your version of the CUDA driver. **Do"
" not install a cuda version smaller than 11.8, preferably between 11.8 "
"and 12.1.**"
msgstr ""

#: ../../source/getting_started/troubleshooting.rst:61
msgid ""
"Say if your CUDA driver version is 11.8, then you can install PyTorch "
"with the following command:"
msgstr ""

#~ msgid ""
#~ "Go to `https://pytorch.org <https://pytorch.org>`_"
#~ " to install a PyTorch version that"
#~ " has been compiled with your version"
#~ " of the CUDA driver."
#~ msgstr ""

